{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63bb80ae",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how we built our simple baseline model, as well as how it was evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "498afc9c-6ab1-4247-ac4a-d532ccd8646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.sparse\n",
    "import json\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import os\n",
    "BASE_DATASET_PATH = '../nngen/data/'\n",
    "DIFF_FILE_SUFFIX = '.diff'\n",
    "COMMIT_FILE_SUFFIX = '.msg'  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3342a",
   "metadata": {},
   "source": [
    "### Data Processing and Loading\n",
    "We use the NNGen dataset from to train our model. We preprocess the commit messages to remove punctuations, digits and convert them to lower cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad80357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_word(word):\n",
    "    word = word.lower()\n",
    "    word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "    word = word.translate(str.maketrans('', '', string.digits))\n",
    "    return word\n",
    "\n",
    "def read_data( split_name):\n",
    "    with open(os.path.join(BASE_DATASET_PATH, split_name+DIFF_FILE_SUFFIX), 'r') as diff_file, open(os.path.join(BASE_DATASET_PATH, split_name+COMMIT_FILE_SUFFIX), 'r') as commit_file:\n",
    "        diff_lines = diff_file.readlines()\n",
    "        diff_lines = [diff.strip() for diff in diff_lines]\n",
    "        commit_lines = commit_file.readlines()\n",
    "        commit_words = [line.strip().split() for line in commit_lines]\n",
    "        commit_words = [word for line in commit_words for word in line]\n",
    "        commit_words = [' '.join(word for word in commit_words)]\n",
    "        return diff_lines, commit_lines\n",
    "    \n",
    "train_diff, train_commit = read_data('cleaned.train')\n",
    "valid_diff, valid_commit = read_data('cleaned.valid')\n",
    "test_diff, test_commit = read_data('cleaned.test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "557a96c3-17b6-4f63-8857-ed15608d6c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_diff: 22112\n",
      "valid_diff: 2511\n",
      "test_diff: 2521\n",
      "mmm a / INSTALL <nl> ppp b / INSTALL <nl> For installation instructions see the manual in the docs subdirectory <nl> - or online at < http : / / grails . codehaus . org / Installation > . <nl> + or online at < http : / / grails . org / Installation > . <nl>\n",
      "updated url of install doc \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_diff: {len(train_diff)}\")\n",
    "print(f\"valid_diff: {len(valid_diff)}\")\n",
    "print(f\"test_diff: {len(test_diff)}\")\n",
    "print(train_diff[42])\n",
    "print(train_commit[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fc1b16-f388-430e-81af-e8d9543c2ec6",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "For baseline model, we use the NNGen model[(Paper)](https://dl.acm.org/doi/pdf/10.1145/3238147.3238190?casa_token=PQjtlNRBvJgAAAAA:dGLvlol87sT5a8biu2oEV9g5HWucpTiHaPZma8Iy1T3DNWCPQEEvupzyQK7mtg7WYRfn2SB_xSlu), whicj is a similarity-based simple statistical model that doesn't require any training. \n",
    "\n",
    "Given a new diff, NNGen first finds the diff which is most similar to it at the token level from the training set, then simply\n",
    "outputs the commit message of the training diff as the generated commit message. \n",
    "1. The approach first extracts diffs from the training set.\n",
    "2. The training diffs and the new diff are represented as vectors in the form of “bags of words”\n",
    "3. Then the cosine similarity between the new diff vector and each training diff vector are calculated\n",
    "4. Top k training diffs with highest similarity scores are picked\n",
    "5. BLEU-4 score between the new diff and each of the top-k training diffs are computed. Training diff with the highest BLEU-4 score is regarded as the nearest neighbor of the new diff. \n",
    "6. Finally, the approach simply outputs the reference message of the nearest neighbor as the final result.\n",
    "\n",
    "To reproduce this algoeithm, we adapted code from [(this repo)](https://github.com/vladislavneon/autogenerating-commit-messages/blob/master/nngen/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d7012d-6303-4fd0-9927-7fda6d6fb569",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a one time run cell. Once the assets are generated, proceed from the following cells\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\S+', stop_words=[''], min_df=8)\n",
    "bow_matrix = vectorizer.fit_transform(train_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58a9ef97-2e71-4bc0-81d5-684e6adc6c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3844\n",
      "Vocabulry size: 3844\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {}\n",
    "for k, v in vectorizer.vocabulary_.items():\n",
    "    try: \n",
    "        vocabulary[k] = v\n",
    "    except Exception as e:\n",
    "        continue\n",
    "print(len(vectorizer.vocabulary_))\n",
    "print(f\"Vocabulry size: {len(vocabulary)}\")\n",
    "scipy.sparse.save_npz('train_bow_matrix.npz', bow_matrix)\n",
    "with open('train_vocabulary.json', 'w') as ouf:\n",
    "    ouf.write(json.dumps(vocabulary, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dde08a20-6dca-4cca-a950-2428aa8ce305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbor(simi, diffs, test_diff, candidate :int =5) -> int:\n",
    "    \"\"\"Find the nearest neighbor using cosine simialrity and bleu score\"\"\"\n",
    "    candidates = simi.argsort()[-candidate:][::-1]\n",
    "    max_score = 0\n",
    "    max_idx = 0\n",
    "    for j in candidates:\n",
    "        score = sentence_bleu([diffs[j].split()], test_diff.split())\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            max_idx = j\n",
    "    return max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a56ed484-d882-45d9-8379-3022be3154c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load vocabulary\n",
    "with open('train_vocabulary.json', 'r') as inf:\n",
    "    vocabulary = json.load(inf) \n",
    "vectorizer = CountVectorizer(vocabulary=vocabulary, token_pattern=r'\\S+', stop_words=['<nl>'])\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "train_bow_matrix = scipy.sparse.load_npz('train_bow_matrix.npz')\n",
    "\n",
    "# convert test data to bow matrix and calculate cosine similarity\n",
    "test_bow_matrix = vectorizer.transform(test_diff)\n",
    "similarities = cosine_similarity(test_bow_matrix, train_bow_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46019ffa-a541-4b86-88d5-fde72d59f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate commit messages based on a nearest neighbor search\n",
    "test_msgs = []\n",
    "for idx, test_simi in enumerate(similarities):\n",
    "    if (idx + 1) % 500 == 0:\n",
    "        print(idx+1)\n",
    "    max_idx = find_nearest_neighbor(test_simi, train_diff, test_diff[idx], candidate=5)\n",
    "    test_msgs.append(train_commit[max_idx])\n",
    "with open('nn_test_msgs.txt', 'w') as ouf:\n",
    "    ouf.write('\\n'.join(test_msgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e859a0f",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "The model is evaluated using BLEU-4 score, which is a standard metric for evaluating the quality of machine translation systems. We use a perl script from [(this repo)](https://github.com/karpathy/neuraltalk/tree/master/eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987b8821",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./bleu.perl <PATH_TO_REFERENCE> < nn_test_msgs.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs505",
   "language": "python",
   "name": "cs505"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
